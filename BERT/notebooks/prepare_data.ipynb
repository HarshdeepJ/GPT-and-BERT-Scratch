{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "VOCAB_SIZE = 10000\n",
    "MASK_PROB = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[A-Z\\s]+:', '', text, flags = re.MULTILINE)\n",
    "    text = re.sub(r'^[A-Z\\s]+$', '', text, flags = re.MULTILINE)\n",
    "    text = ' '.join(text.replace('\\n', ' ').split())\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word_to_idx, vocab_size):\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r\"[\\w']+|.,!?;]\", text)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        unk_id = self.word_to_idx.get('[UNK]')\n",
    "        return [self.word_to_idx.get(token, unk_id) for token in tokens]\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, sentences, vocab_size):\n",
    "        word_counts = Counter()\n",
    "        for sentence in sentences:\n",
    "            word_counts.update(re.findall(r\"[\\w']+|[.,!?;]\", sentence))\n",
    "        special_token = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        word_to_idx = {token: i for i, token in enumerate(special_token)}\n",
    "\n",
    "        most_common_words = word_counts.most_common(vocab_size - len(special_token))\n",
    "        for word, _ in most_common_words:\n",
    "            if word not in word_to_idx:\n",
    "                word_to_idx[word] = len(word_to_idx)\n",
    "        \n",
    "        return cls(word_to_idx, vocab_size)\n",
    "\n",
    "def create_sentence_pairs(sentences):\n",
    "    pairs = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences -1):\n",
    "        sent_a = sentences[i]\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            sent_b = sentences[i+1]\n",
    "            is_next = 0\n",
    "        \n",
    "        else:\n",
    "            random_idx = random.randint(0, num_sentences - 1)\n",
    "            while random_idx == i or random_idx == i+1:\n",
    "                random_idx = random.randomint(0, num_sentences - 1)\n",
    "            sent_b = sentences[random_idx]\n",
    "            is_next = 1\n",
    "        pairs.append((sent_a, sent_b, is_next))\n",
    "    return pairs\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def create_training_example(pair, tokenizer, max_len, mask_prob):\n",
    "    sent_a, sent_b, nsp_label = pair\n",
    "    tokens_a = tokenizer.tokenize(sent_a)\n",
    "    tokens_b = tokenizer.tokenize(sent_b)\n",
    "\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_len - 3)\n",
    "    tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "\n",
    "    segment_ids = [0]*(len(tokens_a) + 2) + [1]*(len(tokens_b) + 1)\n",
    "\n",
    "    mlm_input_tokens = list(tokens)\n",
    "    mlm_labels = [-100] * max_len\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in ['[CLS]', '[SEP]']:\n",
    "            continue\n",
    "\n",
    "        if random.random() < mask_prob:\n",
    "            original_token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "            mlm_labels[i] = original_token_id\n",
    "\n",
    "            if random.random() < 0.8:\n",
    "                mlm_input_tokens[i] = '[MASK]'\n",
    "            elif random.random() < 0.5:\n",
    "                random_word_id = random.randint(len(tokenizer.word_to_idx._fields), tokenizer.vocab_size - 1)\n",
    "                mlm_input_tokens[i] = token.idx_to_word[random_word_id]\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(mlm_input_tokens)\n",
    "        padding_len = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * padding_len)\n",
    "        segment_ids.extend([0] * padding_len)\n",
    "\n",
    "        attention_mask = [1] * (len(tokens)) + [0] * padding_len\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype = torch.long),\n",
    "            'segment_ids': torch.tensor(segment_ids, dtype = torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype = torch.long),\n",
    "            'mlm_labels': torch.tensor(mlm_labels, dtype = torch.long),\n",
    "            'nsp_label': torch.tensor(nsp_label, dtype = torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    project_root = Path(__file__).parent.parent\n",
    "    raw_data_path = project_root / 'input.txt'\n",
    "    processed_data_dir = project_root / 'data' / 'processed'\n",
    "    tokenizer_path = project_root / 'data' / 'tokenizer.pkl'\n",
    "\n",
    "    processed_data_dir.mkdir(parents = True, exist_ok = True)\n",
    "    print('Step 1: Reading and Cleaning Text: ')\n",
    "    with open(raw_data_path, 'r', encoding = 'utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    cleaned_text = clean_text(raw_text)\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except nltk.downloader.DownloadError:\n",
    "        print(\"Download NLTK 'punkt' model...\")\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    print(f\"   ...found {len(sentences)} sentences.\")\n",
    "\n",
    "    print(f'Step 2: Building vocabulary (size = {VOCAB_SIZE}) and tokenizer...')\n",
    "    tokenizer = Tokenizer.build(sentences, VOCAB_SIZE)\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(f'   ...vocabulary and tokenizer saved to {tokenizer_path}')\n",
    "\n",
    "    print(\"Step 3: Creating sentence pairs for NSP...\")\n",
    "    sentence_pairs = create_sentence_pairs(sentences)\n",
    "    print(f'    ...created {len(sentence_pairs)} pairs.')\n",
    "\n",
    "    print('Step 4: Generating and saving training examples...')\n",
    "    training_examples = []\n",
    "    for pair in tqdm(sentence_pairs, desc = 'Creating examples'):\n",
    "        example = create_training_example(pair, tokenizer, MAX_SEQ_LEN, MASK_PROB)\n",
    "        training_examples.append(example)\n",
    "    \n",
    "    output_file = processed_data_dir / 'bert_data.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(training_examples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     project_root \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m      3\u001b[0m     raw_data_path \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m     processed_data_dir \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
